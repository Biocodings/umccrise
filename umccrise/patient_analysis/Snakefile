""" UMCCR post-bcbio patient analysis workflow
"""
import os
import sys
from os.path import join, abspath, dirname, isfile
from ngs_utils.file_utils import splitext_plus
from ngs_utils.bcbio import BcbioProject
from ngs_utils.file_utils import add_suffix, get_ungz_gz
from ngs_utils.logger import critical
from umccrise.utils import get_loc


shell.executable("bash")
shell.prefix("")


pcgr_url = config.get('pcgr_url', 'ec2-13-55-18-20')
cov_by_phenotype = config.get('cov_by_phenotype', {'tumor': 30, 'normal': 10})  # For goleft regions coverage, use minimum coverage 10 for normal, 30 for tumor
threads_max = 32  # Use up to 32 cores at once, if available

run = BcbioProject(config.get('run_dir', abspath(os.getcwd())))  

loc = get_loc()
az300 = join(loc.hsapiens, run.genome_build, 'coverage', 'prioritize', 'cancer', 'az300.bed.gz')
ref_fa = join(loc.hsapiens, run.genome_build, 'seq', run.genome_build + '.fa')

# Batch objects index by tumor sample names
batch_by_name = {b.tumor.name: b for b in run.batch_by_name.values() if not b.is_germline()}
name = config.get('batch') or config.get('sample')
if name:
    batch_by_name = {n: b for n, b in batch_by_name.items() if b.name == name or b.tumor.name == name}
    if len(batch_by_name) == 0:
        critical(f'Error: could not find a batch or a sample with the name {name}')


# Generating unique ID for PCGR tarballs
if 'unique_id' in config:
    unique_id = config['unique_id']
    print(f'Reusing unique ID for PCGR (for downloading): {unique_id}')
else:
    import uuid
    unique_id = str(uuid.uuid4().hex[:6])
    print(f'Creating new unique ID for PCGR: {unique_id}')


def upload_all(wc):
    batch_outputs = [
        '{batch}/coverage/indexcov/index.html',
        '{batch}/coverage/tumor.depth.bed',
        '{batch}/coverage/normal.depth.bed',
        '{batch}/structural/cnvkit-diagram.pdf',
        '{batch}/structural/sv-prioritize-manta-pass.vcf',
        '{batch}/structural/sv-prioritize-manta-pass.bedpe',
        '{batch}/structural/sv-prioritize-manta-pass.ribbon.bed',
        '{batch}/igv/tumor_mini.bam',
        '{batch}/igv/normal_mini.bam',
        '{batch}/rmd_report.html',
        '{batch}/pcgr/input/{batch}-' + unique_id + '-somatic.tar.gz',
        '{batch}/pcgr/input/{batch}-' + unique_id + '-germline.tar.gz',
    ]
    all = expand(batch_outputs, batch=batch_by_name.keys()) + [
        'log/data_versions.csv',
        'log/programs.txt',
        'log/config',
        'multiqc_report.html'
    ]
    if loc.name == 'spartan':
        all.extend(expand([
            '{batch}/pcgr/input/upload-somatic.done',
            '{batch}/pcgr/input/upload-germline.done',
        ], batch=batch_by_name.keys()))
    return all

rule all:
    input: upload_all


include: "Snakefile.small_variants"
include: "Snakefile.coverage"
include: "Snakefile.pcgr"
include: "Snakefile.structural"
include: "Snakefile.igv"
include: "Snakefile.rmd"


rule copy_multiqc:  # {}
    input:
        join(run.date_dir, 'multiqc/multiqc_report.html')
    output:
        'multiqc_report.html'
    shell:
        'cp {input} {output}'


## Additional information
# TODO: link it to MultiQC
rule copy_logs:  # {}
    input:
        join(run.date_dir, 'data_versions.csv'), 
        join(run.date_dir, 'programs.txt'), 
        run.config_dir
    output:
        'log/data_versions.csv', 
        'log/programs.txt', 
        'log/config'
    params:
        log_dir = 'log'
    shell:
        'cp -r {input} {params.log_dir}'



