""" UMCCR post-bcbio patient analysis workflow
"""
import os
import sys
from os.path import join, abspath, dirname, isfile, basename, splitext
from ngs_utils.file_utils import splitext_plus
from ngs_utils.bcbio import BcbioProject
from ngs_utils.file_utils import add_suffix, get_ungz_gz
from ngs_utils.logger import critical
from python_utils.hpc import get_ref_file, get_loc


shell.executable("bash")
shell.prefix("")


pcgr_url = config.get('pcgr_url', 'ec2-13-55-18-20')
cov_by_phenotype = config.get('cov_by_phenotype', {'tumor': 30, 'normal': 10})  # For goleft regions coverage, use minimum coverage 10 for normal, 30 for tumor
threads_max = 32  # Use up to 32 cores at once, if available

run = BcbioProject(config.get('run_dir', abspath(os.getcwd())))
project_id = splitext(basename(run.bcbio_yaml_fpath))[0]


az300 = get_ref_file(run.genome_build, 'az300')
ref_fa = get_ref_file(run.genome_build)

# Batch objects index by tumor sample names
batch_by_name = {b.tumor.name: b for b in run.batch_by_name.values() if not b.is_germline()}
name = config.get('batch') or config.get('sample')
if name:
    batch_by_name = {n: b for n, b in batch_by_name.items() if b.name == name or b.tumor.name == name}
    if len(batch_by_name) == 0:
        critical(f'Error: could not find a batch or a sample with the name {name}')


# Generating unique ID for PCGR tarballs
if 'unique_id' in config:
    unique_id = config['unique_id']
    print(f'Reusing unique ID for PCGR (for downloading): {unique_id}')
else:
    import uuid
    unique_id = str(uuid.uuid4().hex[:6])
    print(f'Creating new unique ID for PCGR: {unique_id}')


def upload_all(wc):
    batch_outputs = [
        '{batch}/coverage/{batch}-indexcov/index.html',
        '{batch}/coverage/{batch}-tumor.depth.bed',
        '{batch}/coverage/{batch}-normal.depth.bed',
        '{batch}/structural/{batch}-cnvkit-diagram.pdf',
        '{batch}/structural/{batch}-sv-prioritize-manta-pass.vcf',
        '{batch}/structural/{batch}-sv-prioritize-manta-pass.bedpe',
        '{batch}/structural/{batch}-sv-prioritize-manta-pass.ribbon.bed',
        '{batch}/igv/{batch}-tumor_mini.bam',
        '{batch}/igv/{batch}-normal_mini.bam',
        '{batch}/{batch}-rmd_report.html',
        '{batch}/pcgr/input/{batch}-' + unique_id + '-somatic.tar.gz',
        '{batch}/pcgr/input/{batch}-' + unique_id + '-normal.tar.gz',
    ]
    all = expand(batch_outputs, batch=batch_by_name.keys()) + [
        'log/' + project_id + '-data_versions.csv',
        'log/' + project_id + '-programs.txt',
        'log/' + project_id + '-config',
        project_id + '-multiqc_report.html'
    ]
    if get_loc().name == 'spartan':
        all.extend(expand([
            '{batch}/pcgr/input/upload-somatic.done',
            '{batch}/pcgr/input/upload-normal.done',
        ], batch=batch_by_name.keys()))
    return all

rule all:
    input: upload_all


include: "Snakefile.small_variants"
include: "Snakefile.coverage"
include: "Snakefile.pcgr"
include: "Snakefile.structural"
include: "Snakefile.igv"
include: "Snakefile.rmd"


rule copy_multiqc:  # {}
    input:
        join(run.date_dir, 'multiqc/multiqc_report.html')
    output:
        project_id + '-multiqc_report.html'
    shell:
        'cp {input} {output}'


## Additional information
# TODO: link it to MultiQC
rule copy_logs:  # {}
    input:
        join(run.date_dir, 'data_versions.csv'), 
        join(run.date_dir, 'programs.txt'), 
        run.config_dir
    output:
        'log/' + project_id + '-data_versions.csv',
        'log/' + project_id + '-programs.txt',
        'log/' + project_id + '-config'
    shell:
        'cp -r {input[0]} {output[0]} && cp -r {input[1]} {output[1]} && cp -r {input[2]} {output[2]}'



