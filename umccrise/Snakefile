""" UMCCR post-bcbio patient analysis workflow
"""
import os
import sys
from os.path import join, abspath, dirname, isfile, basename, splitext
from ngs_utils.file_utils import splitext_plus
from ngs_utils.bcbio import BcbioProject
from ngs_utils.file_utils import add_suffix, get_ungz_gz
from ngs_utils.logger import critical
from python_utils.hpc import get_ref_file, get_loc


shell.executable("bash")
shell.prefix("")


pcgr_url = config.get('pcgr_url', 'ec2-13-55-18-20')
cov_by_phenotype = config.get('cov_by_phenotype', {'tumor': 30, 'normal': 10})  # For goleft regions coverage, use minimum coverage 10 for normal, 30 for tumor
threads_max = 32  # Use up to 32 cores at once, if available

run = BcbioProject(config.get('run_dir', abspath(os.getcwd())))
project_id = splitext(basename(run.bcbio_yaml_fpath))[0]


az300 = get_ref_file(run.genome_build, 'az300')
ref_fa = get_ref_file(run.genome_build)

# Batch objects index by tumor sample names
batch_by_name = {b.tumor.name: b for b in run.batch_by_name.values() if not b.is_germline()}
name = config.get('batch') or config.get('sample')
if name:
    selected_batch_by_name = {n: b for n, b in batch_by_name.items() if b.name == name or b.tumor.name == name}
    if len(selected_batch_by_name) == 0:
        critical(f'Error: could not find a batch or a sample with the name {name}. '
                 f'Known batches: {list(b.name for b in batch_by_name.values())}, '
                 f'known samples: {list(b.tumor.name for b in batch_by_name.values())}')
    batch_by_name = selected_batch_by_name


# Generating unique ID for PCGR tarballs
if 'unique_id' in config:
    uuid_suffix = '.' + config['unique_id']
    print(f'Reusing unique ID for PCGR (for downloading): {unique_id}')
else:
    uuid_suffix = ''


is_spartan = get_loc().name == 'spartan'
is_raijin = get_loc().name == 'raijin'
is_hpc = is_spartan or is_raijin
upload_proxy = ''
if is_spartan:
    upload_proxy = 'HTTPS_PROXY=http://wwwproxy.unimelb.edu.au:8000 '


rule all:
    input: 'umccrised.done'
    # A trick to avoid duplicating all input paths in the top "all" rule which has to be defined on top.

# TODO: try subworkflows here? http://snakemake.readthedocs.io/en/stable/snakefiles/modularization.html#sub-workflows
"""
subworkflow small_variants:
    workdir: 'small_variants'
    snakefile: 'Snakefile.small_variants'

rule all:
    input:  small_variants('small_variants.done')
    output: ...
    shell:  ...
"""
# Or maybe it's not reasonable and not doable here since the input file is a phony .done, and also we depend on config in subworkflows


include: "Snakefile.small_variants"
include: "Snakefile.coverage"
include: "Snakefile.structural"
include: "Snakefile.igv"
include: "Snakefile.pcgr"
include: "Snakefile.rmd"


rule copy_multiqc:  # {}
    input:
        join(run.date_dir, 'multiqc/multiqc_report.html')
    output:
        project_id + '-multiqc_report.html'
    shell:
        'cp {input} {output}'


## Additional information
# TODO: link it to MultiQC
rule copy_logs:  # {}
    input:
        join(run.date_dir, 'data_versions.csv'), 
        join(run.date_dir, 'programs.txt'), 
        run.config_dir
    output:
        'log/' + project_id + '-data_versions.csv',
        'log/' + project_id + '-programs.txt',
        'log/' + project_id + '-config'
    shell:
        'cp -r {input[0]} {output[0]} && cp -r {input[1]} {output[1]} && cp -r {input[2]} {output[2]}'


rule umccrise:
    input:  # Copy here inputs of the "milestone" rules (rules without output defined in the end of each Snakemake.* file)
        rules.copy_multiqc.output,
        rules.copy_logs.output,
        rules.coverage.output,
        rules.structural.output,
        rules.small_variants.output,
        rules.rmd.output,
        rules.pcgr.output,
        rules.igv.output,
        (rules.pcgr_download.output if is_hpc else [])
    output:
        temp(touch('umccrised.done'))
