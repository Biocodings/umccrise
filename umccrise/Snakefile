""" UMCCR post-bcbio patient analysis workflow
"""
import glob
import os
import sys
import multiprocessing
from os.path import join, abspath, dirname, isfile, basename, splitext
from ngs_utils.file_utils import splitext_plus, verify_file, verify_dir, adjust_path

from ngs_utils.Sample import BaseSample, Batch
from ngs_utils.bcbio import BcbioProject
from natsort import natsort_keygen

from ngs_utils.utils import flatten
from ngs_utils.logger import critical, info, debug, warn
from ngs_utils import logger as ngs_utils_logger
from hpc_utils import hpc
from ngs_utils.utils import set_locale; set_locale()

shell.executable(os.environ.get('SHELL', 'bash'))
shell.prefix("")

within_sub_workflow = 'within_sub_workflow' in config
config['within_sub_workflow'] = 'yes'  # to avoid redundant logging in cluster sub-executions of the Snakefile

py_path = sys.executable  # /miniconda/envs/umccrise/bin/python
env_path = dirname(dirname(py_path))  # /miniconda/envs/umccrise
conda_cmd = 'export PATH=' + env_path + '_{}/bin:$PATH; '


class DragenSample(BaseSample):
    natsort_key = natsort_keygen()

    def __init__(self, **kwargs):
        BaseSample.__init__(self, **kwargs)  # name, dirpath, work_dir, bam, vcf, phenotype, normal_match


class DragenBatch(Batch):
    def find_somatic_vcf(self, silent=False):
        vcf_fpath_gz = adjust_path(join(self.parent_project.dir, self.name + '.hard-filtered.vcf.gz'))
        if isfile(vcf_fpath_gz):
            verify_file(vcf_fpath_gz, is_critical=True)
            if not silent: info(f'Found somatic VCF in <dragen-dir>/<tumor-name>-hard-filtered.vcf.gz: ' + vcf_fpath_gz)
            self.somatic_vcf = vcf_fpath_gz


class DragenProject:
    def __init__(self, input_dir=None):
        self.input_dir = input_dir
        self.project_name = None
        self.samples = []
        self.batch_by_name = dict()
        for fpath in glob.glob(join(self.input_dir, '*.hard-filtered.vcf.gz')):
            batch_name = fpath.split('.hard-filtered.vcf.gz')[0]
            batch = Batch(batch_name, parent_project=self)
            batch.tumor = DragenSample(name=batch_name, phenotype='tumor')
            batch.normal = DragenSample(name=batch_name + ' normal', phenotype='normal')
            self.samples.extend([batch.tumor, batch.normal])
            batch.tumor.bam = join(self.input_dir, batch_name + '_tumor.bam')
            batch.normal.bam = join(self.input_dir, batch_name + '.bam')
            batch.tumor.vcf = join(self.input_dir, '.hard-filtered.vcf.gz')
            self.batch_by_name[batch_name] = batch


def prep_inputs(config_):
    ###############################
    #### Parsing a bcbio or Dragen project ####
    ###############################

    # Parsing a bcbio or Dragen project and including/excluding samples
    include_names = config_.get('batch') or config_.get('sample')
    if include_names:
        include_names = str(include_names).split(',')
        include_names = [v for v in flatten([sn.split('__') for sn in include_names])]  # support "batch__sample" notation
    exclude_names = config_.get('exclude')
    if exclude_names:
        exclude_names = str(exclude_names).split(',')
        exclude_names = [v for v in flatten([sn.split('__') for sn in exclude_names])]  # support "batch__sample" notation

    if config_.get('debug', 'no') == 'yes':
        ngs_utils_logger.is_debug = True

    ngs_utils_logger.is_silent = within_sub_workflow  # to avoid redundant logging in cluster sub-executions of the Snakefile
    input_path = config_.get('bcbio_project', abspath(os.getcwd()))
    if glob.glob(join(input_path, '*-replay.json')):
        run = DragenProject(input_path)
        ngs_utils_logger.is_silent = False
        batch_by_name = run.batch_by_name

    else:
        run = BcbioProject(input_path,
                           exclude_samples=exclude_names,
                           include_samples=include_names)
        ngs_utils_logger.is_silent = False
        run.project_name = splitext(basename(run.bcbio_yaml_fpath))[0]

        if len(run.batch_by_name) == 0:
            if exclude_names:
                critical(f'Error: no samples left with the exclusion of batch/sample name(s): {", ".join(exclude_names)}.'
                         f'Check yaml file for available options: {run.bcbio_yaml_fpath}.')
            if include_names:
                critical(f'Error: could not find a batch or a sample with the name(s): {", ".join(include_names)}. '
                         f'Check yaml file for available options: {run.bcbio_yaml_fpath}')
            critical(f'Error: could not parse any batch or samples in the bcbio project. '
                     f'Please check the bcbio yaml file: {run.bcbio_yaml_fpath}')

        # Batch objects index by tumor sample names
        batches = [b for b in run.batch_by_name.values() if not b.is_germline() and b.tumor and b.normal]
        assert batches
        batch_by_name = {b.name + '__' + b.tumor.name: b for b in batches}

    # Reference files
    if config_.get('genomes_dir'):
        hpc.set_genomes_dir(config_.get('genomes_dir'))

    return run, batch_by_name


run, batch_by_name = prep_inputs(config)

is_spartan = hpc.name == 'spartan'
is_raijin = hpc.name == 'raijin'
is_hpc = is_spartan or is_raijin
upload_proxy = ''
if is_spartan:
    upload_proxy = 'HTTPS_PROXY=http://wwwproxy.unimelb.edu.au:8000 '

upload_igv = is_hpc and config.get('upload_igv', 'no') == 'yes'


threads_on_node = hpc.threads_on_node or multiprocessing.cpu_count() or 1
total_cores = config.get('total_cores', 128)
threads_per_batch  = max(1, min(threads_on_node, total_cores // len(batch_by_name)))
threads_per_sample = max(1, min(threads_on_node, total_cores // len(run.samples)))
print(f'Threads available on the machine: {threads_on_node}, total cores allowed: {total_cores}. '
      f'Batches found: {threads_per_batch}, using {threads_per_batch} threads per batch, {threads_per_sample} per sample')

is_ffpe = run.somatic_caller == 'strelka2'


rule all:
    input: 'all.done'
    # A trick to avoid duplicating all input paths in the top "all" rule which has to be defined on top.


# TODO: try subworkflows here? http://snakemake.readthedocs.io/en/stable/snakefiles/modularization.html#sub-workflows
"""
subworkflow small_variants:
    workdir: 'small_variants'
    snakefile: 'Snakefile.small_variants'

rule all:
    input:  small_variants('all.done')
    output: ...
    shell:  ...
"""
# Or maybe it's not reasonable and not doable here since the input file is a phony .done, and also we depend on config in subworkflows


include: "small_variants.smk"
include: "structural.smk"
include: "purple.smk"
include: "coverage.smk"
include: "pcgr.smk"
include: "rmd.smk"
include: "igv.smk"
include: "conpair.smk"
include: "multiqc.smk"


localrules: umccrise


rule umccrise:
    input:  # Copy here inputs of the "milestone" rules (rules without output defined in the end of each Snakemake.* file)
        rules.multiqc.output,
        rules.coverage.output,
        rules.structural.output,
        rules.small_variants.output,
        rules.purple.output,
        rules.pcgr.output,
        rules.rmd.output,
        (rules.igv.output if config.get('upload_igv', 'no') == 'yes' else []),
    output:
        touch('all.done')
