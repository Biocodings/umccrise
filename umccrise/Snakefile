""" UMCCR post-bcbio patient analysis workflow
"""
import os
import sys
from os.path import join, abspath, dirname, isfile, basename, splitext
from ngs_utils.file_utils import splitext_plus, verify_file, verify_dir
from ngs_utils.bcbio import BcbioProject
from ngs_utils.utils import flatten
from ngs_utils.logger import critical, info, debug
from ngs_utils import logger as ngs_utils_logger
from hpc_utils import hpc
from ngs_utils.utils import set_locale; set_locale()

shell.executable(os.environ.get('SHELL', 'bash'))
shell.prefix("")

within_sub_workflow = 'within_sub_workflow' in config
config['within_sub_workflow'] = 'yes'  # to avoid redundant logging in cluster sub-executions of the Snakefile

py_path = sys.executable  # /miniconda/envs/umccrise/bin/python
env_path = dirname(dirname(py_path))  # /miniconda/envs/umccrise
conda_cmd = 'export PATH=' + env_path + '_{}/bin:$PATH; '


def prep_inputs(config_):

    ###############################
    #### Parsing bcbio project ####
    ###############################

    # Parsing bcbio project and including/excluding samples
    include_names = config_.get('batch') or config_.get('sample')
    if include_names:
        include_names = str(include_names).split(',')
        include_names = [v for v in flatten([sn.split('__') for sn in include_names])]  # support "batch__sample" notation
    exclude_names = config_.get('exclude')
    if exclude_names:
        exclude_names = str(exclude_names).split(',')
        exclude_names = [v for v in flatten([sn.split('__') for sn in exclude_names])]  # support "batch__sample" notation

    if config_.get('debug', 'no') == 'yes':
        ngs_utils_logger.is_debug = True

    ngs_utils_logger.is_silent = within_sub_workflow  # to avoid redundant logging in cluster sub-executions of the Snakefile
    run = BcbioProject(config_.get('bcbio_project', abspath(os.getcwd())),
                       exclude_samples=exclude_names,
                       include_samples=include_names)
    ngs_utils_logger.is_silent = False
    run.project_name = splitext(basename(run.bcbio_yaml_fpath))[0]

    if len(run.batch_by_name) == 0:
        if exclude_names:
            critical(f'Error: no samples left with the exclusion of batch/sample name(s): {", ".join(exclude_names)}.'
                     f'Check yaml file for available options: {run.bcbio_yaml_fpath}.')
        if include_names:
            critical(f'Error: could not find a batch or a sample with the name(s): {", ".join(include_names)}. '
                     f'Check yaml file for available options: {run.bcbio_yaml_fpath}')
        critical(f'Error: could not parse any batch or samples in the bcbio project. '
                 f'Please check the bcbio yaml file: {run.bcbio_yaml_fpath}')

    # Batch objects index by tumor sample names
    batches = [b for b in run.batch_by_name.values() if not b.is_germline() and b.tumor and b.normal]
    assert batches

    batch_by_name = {b.name + '__' + b.tumor.name: b for b in batches}

    # Reference files
    if config_.get('genomes_dir'):
        hpc.genomes_dir = config_.get('genomes_dir')

    return run, batch_by_name


run, batch_by_name = prep_inputs(config)

is_spartan = hpc.name == 'spartan'
is_raijin = hpc.name == 'raijin'
is_hpc = is_spartan or is_raijin
upload_proxy = ''
if is_spartan:
    upload_proxy = 'HTTPS_PROXY=http://wwwproxy.unimelb.edu.au:8000 '

upload_igv = is_hpc and config.get('no_s3', 'no') == 'no'


threads_on_node = hpc.threads_on_node or 1
total_cores = config.get('total_cores', 128)
threads_per_batch  = max(1, min(threads_on_node, total_cores // len(batch_by_name)))
threads_per_sample = max(1, min(threads_on_node, total_cores // len(run.samples)))
print(f'Threads on node: {threads_on_node}, total cores allowed: {total_cores}. '
      f'Batches found: {threads_per_batch}, using {threads_per_batch} threads per batch, {threads_per_sample} per sample')

is_ffpe = run.somatic_caller == 'strelka2'


rule all:
    input: 'all.done'
    # A trick to avoid duplicating all input paths in the top "all" rule which has to be defined on top.


# TODO: try subworkflows here? http://snakemake.readthedocs.io/en/stable/snakefiles/modularization.html#sub-workflows
"""
subworkflow small_variants:
    workdir: 'small_variants'
    snakefile: 'Snakefile.small_variants'

rule all:
    input:  small_variants('all.done')
    output: ...
    shell:  ...
"""
# Or maybe it's not reasonable and not doable here since the input file is a phony .done, and also we depend on config in subworkflows


include: "small_variants.smk"
include: "structural.smk"
include: "purple.smk"
include: "coverage.smk"
include: "pcgr.smk"
include: "rmd.smk"
include: "igv.smk"
include: "conpair.smk"
include: "multiqc.smk"


localrules: umccrise



rule umccrise:
    input:  # Copy here inputs of the "milestone" rules (rules without output defined in the end of each Snakemake.* file)
        rules.multiqc.output,
        rules.coverage.output,
        rules.structural.output,
        rules.small_variants.output,
        rules.purple.output,
        rules.pcgr.output,
        rules.rmd.output,
        (rules.igv.output if config.get('no_igv', 'no') == 'no' else []),
    output:
        touch('all.done')
