""" UMCCR post-bcbio patient analysis workflow
"""
import os
import sys
from os.path import join, abspath, dirname, isfile, basename, splitext
from ngs_utils.file_utils import splitext_plus, verify_file, verify_dir
from ngs_utils.bcbio import BcbioProject
from ngs_utils.utils import flatten
from ngs_utils.logger import critical, info, debug
from ngs_utils import logger as ngs_utils_logger
from hpc_utils.hpc import get_loc, find_loc, get_ref_file as hpc_get_ref_file
from ngs_utils.utils import set_locale; set_locale()

shell.executable(os.environ.get('SHELL', 'bash'))
shell.prefix("")

loc = find_loc()
is_spartan = loc and loc.name == 'spartan'
is_raijin = loc and loc.name == 'raijin'
is_hpc = is_spartan or is_raijin
upload_proxy = ''
if is_spartan:
    upload_proxy = 'HTTPS_PROXY=http://wwwproxy.unimelb.edu.au:8000 '

upload_igv = is_hpc and config.get('no_s3', 'no') == 'no'

within_sub_workflow = 'within_sub_workflow' in config
config['within_sub_workflow'] = 'yes'  # to avoid redundant logging in cluster sub-executions of the Snakefile

py_path = sys.executable  # /miniconda/envs/umccrise/bin/python
env_path = dirname(dirname(py_path))  # /miniconda/envs/umccrise
conda_cmd = 'export PATH=' + env_path + '_{}/bin:$PATH; '

def get_ref_file(*args, **kwargs):
    return hpc_get_ref_file(*args, **kwargs, genomes_dir=config.get('genomes_dir'))


def prep_inputs(config_):

    ###############################
    #### Parsing bcbio project ####
    ###############################

    # Parsing bcbio project and including/excluding samples
    include_names = config_.get('batch') or config_.get('sample')
    if include_names:
        include_names = str(include_names).split(',')
        include_names = [v for v in flatten([sn.split('__') for sn in include_names])]  # support "batch__sample" notation
    exclude_names = config_.get('exclude')
    if exclude_names:
        exclude_names = str(exclude_names).split(',')
        exclude_names = [v for v in flatten([sn.split('__') for sn in exclude_names])]  # support "batch__sample" notation

    ngs_utils_logger.is_silent = within_sub_workflow  # to avoid redundant logging in cluster sub-executions of the Snakefile
    run = BcbioProject(config_.get('bcbio_project', abspath(os.getcwd())),
                       exclude_samples=exclude_names,
                       include_samples=include_names)
    ngs_utils_logger.is_silent = False
    run.project_name = splitext(basename(run.bcbio_yaml_fpath))[0]

    if len(run.batch_by_name) == 0:
        if exclude_names:
            critical(f'Error: no samples left with the exclusion of batch/sample name(s): {", ".join(exclude_names)}.'
                     f'Check yaml file for available options: {run.bcbio_yaml_fpath}.')
        if include_names:
            critical(f'Error: could not find a batch or a sample with the name(s): {", ".join(include_names)}. '
                     f'Check yaml file for available options: {run.bcbio_yaml_fpath}')
        critical(f'Error: could not parse any batch or samples in the bcbio project. '
                 f'Please check the bcbio yaml file: {run.bcbio_yaml_fpath}')

    # Batch objects index by tumor sample names
    batches = [b for b in run.batch_by_name.values() if not b.is_germline() and b.tumor and b.normal]
    assert batches

    batch_by_name = {b.name + '__' + b.tumor.name: b for b in batches}

    ###########################################################
    #### Reference files                                   ####
    ###########################################################
    ref_fa = get_ref_file(run.genome_build)
    truth_regions = get_ref_file(run.genome_build, key=['hmf_giab_conf'])
    pon_dir = get_ref_file(run.genome_build, key='panel_of_normals_dir')
    pcgr_data = get_ref_file(key='pcgr_data', loc=loc)
    cacao_data = get_ref_file(key='cacao_data', loc=loc)

    ###########################################################
    #### Done looking for refernece files, now some checks ####
    ###########################################################
    truth_regions = verify_file(truth_regions, is_critical=True, description='GiaB truth regions')

    ref_fa = verify_file(ref_fa, is_critical=True, description='Reference fasta')
    verify_file(ref_fa + '.fai', is_critical=True, description='Reference fasta fai index')

    if pon_dir:
        verify_dir(pon_dir, 'Panel of normals directory', is_critical=True)
        verify_file(join(pon_dir, 'panel_of_normals.snps.vcf.gz'), is_critical=True, description='Panel of normals SNPs file in user provided folder')
        verify_file(join(pon_dir, 'panel_of_normals.snps.vcf.gz.tbi'), is_critical=True, description='Please index panel of normal files with tabix')
        verify_file(join(pon_dir, 'panel_of_normals.indels.vcf.gz'), is_critical=True, description='Panel of normals indels file in user provided folder')
        verify_file(join(pon_dir, 'panel_of_normals.indels.vcf.gz.tbi'), is_critical=True, description='Please index panel of normal files with tabix')

    verify_dir(pcgr_data, is_critical=True, description='PCGR data directory')

    return run, batch_by_name, ref_fa, truth_regions, pon_dir, pcgr_data, cacao_data


run, batch_by_name, ref_fa, truth_regions, pon_dir, pcgr_data, cacao_data = prep_inputs(config)


threads_on_node = loc.threads_on_node
total_cores = config.get('total_cores', 128)
threads_per_batch  = max(1, min(threads_on_node, total_cores // len(batch_by_name)))
threads_per_sample = max(1, min(threads_on_node, total_cores // len(run.samples)))
print(f'Threads on node: {threads_on_node}, total cores allowed: {total_cores}. '
      f'Batches found: {threads_per_batch}, using {threads_per_batch} threads per batch, {threads_per_sample} per sample')


rule all:
    input: 'all.done'
    # A trick to avoid duplicating all input paths in the top "all" rule which has to be defined on top.


# TODO: try subworkflows here? http://snakemake.readthedocs.io/en/stable/snakefiles/modularization.html#sub-workflows
"""
subworkflow small_variants:
    workdir: 'small_variants'
    snakefile: 'Snakefile.small_variants'

rule all:
    input:  small_variants('all.done')
    output: ...
    shell:  ...
"""
# Or maybe it's not reasonable and not doable here since the input file is a phony .done, and also we depend on config in subworkflows


include: "small_variants.smk"
include: "coverage.smk"
include: "structural.smk"
include: "purple.smk"
include: "pcgr.smk"
include: "rmd.smk"
include: "igv.smk"
include: "conpair.smk"
include: "multiqc.smk"


localrules: umccrise



rule umccrise:
    input:  # Copy here inputs of the "milestone" rules (rules without output defined in the end of each Snakemake.* file)
        rules.multiqc.output,
        rules.coverage.output,
        rules.structural.output,
        rules.small_variants.output,
        rules.purple.output,
        rules.pcgr.output,
        rules.rmd.output,
        (rules.igv.output if config.get('no_igv', 'no') == 'no' else []),
    output:
        touch('all.done')
